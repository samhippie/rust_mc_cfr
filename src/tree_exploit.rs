use std::collections::{HashMap, HashSet};

use crate::game::{Game, Player};
use crate::cfr::CounterFactualRegret;

pub struct TreeExploit<'a, G: Game> {
    game_constructor: Box<dyn Fn() -> G>,
    cfr: &'a CounterFactualRegret,
    on_player: Player,

    seen_infosets: HashSet<u64>,
}

impl<'a, G: Game + Clone> TreeExploit<'a, G> {
    pub fn new(game_constructor: Box<dyn Fn() -> G>, cfr: &CounterFactualRegret) -> TreeExploit<G> {
        TreeExploit {
            game_constructor,
            cfr,
            on_player: Player::P1,
            seen_infosets: HashSet::new(),
        }
    }

    pub fn run(&mut self) -> (f32, f32) {
        //TODO consider sampling mutliple times to make up for the fact we're not including chance
        self.on_player = Player::P1;
        let game = (*self.game_constructor)();
        self.seen_infosets.clear();
        let value1 = self.search(vec![(game, 1.0)]);

        self.on_player = Player::P2;
        let game = (*self.game_constructor)();
        self.seen_infosets.clear();
        let value2 = self.search(vec![(game, 1.0)]);

        (value1, value2)
    }

    /// Searches the set of games that share an infoset for `self.on_player`
    ///
    /// (Sharing an infoset is not necessary for the off-player, as they're just accepting every action)
    ///
    /// The f32 associated with each game is the off-player's contribution to the game's history reach probability
    ///
    /// (We should include chance's contribution too, but chance is internal to all of our games)
    /// ***
    /// This will only work for games where players alternate, which basically means that a player cannot make an unknown number of moves.
    /// Violating this will mean that you will have the same on-player infoset spread across separate search batches, which will mean that the calculation is incorrect.
    /// If a single history is present in a batch, then that entire history's infoset must also be present. 
    ///
    /// Basically, all histories in an infoset must come from the same opponent state
    /// ***
    /// #TODO
    /// * Add assertions that the same infoset doesn't get spread across batches
    /// * If the results aren't good, make a new game type that makes chance a third player
    fn search(&mut self, games: Vec<(G, f32)>) -> f32 {
        let rewards: Option<Vec<f32>> = games.iter().map(|(g, rp)| {
            match g.get_reward() {
                Some(r) => Some(r * rp),
                None => None,
            }
        }).collect();
        if let Some(rewards) = rewards {
            let reward = rewards.iter().sum::<f32>();
            //if we use chance's reach probability, we need to sum over all chance states
            //let reach = game.get_reach_prob(Some(Player::P1)) * game.get_reach_prob(Some(Player::P2)) * game.get_reach_prob(None);
            return if self.on_player == Player::P1 {
                reward
            } else {
                -1.0 * reward
            };
        }

        let (player, actions) = games[0].0.get_turn();

        return if player == self.on_player {
            self.check_infosets(&games, player);
            //pick the action that maximizes the reward across infosets with probability 1
            let mut best_response = -1.0;
            for action in actions.iter() {
                let mut subgames = vec![];
                for (game, rp) in games.iter() {
                    let mut subgame = game.clone();
                    subgame.take_turn(player, action);
                    subgames.push((subgame, *rp));
                }
                let reward = self.search(subgames);
                if reward > best_response {
                    best_response = reward;
                }
            }
            best_response
        } else {
            //sum of all response values weighted by probability

            //maps infoset to list of subgames in infoset and sum of probabilities for eaching each infoset from the current infoset
            let mut subgame_map: HashMap<u64, Vec<(G, f32)>> = HashMap::new();
            for (game, rp) in games.iter() {
                let infoset = game.get_infoset(player);
                let (_, actions) = game.get_turn();
                let probs = self.cfr.get_avg_strategy(player, &infoset, actions.len()).unwrap();
                for (action, prob) in actions.iter().zip(probs.iter()) {
                    let mut subgame = game.clone();
                    subgame.take_turn(player, action);
                    let new_infoset = subgame.get_infoset(player.other());
                    let entry = subgame_map.entry(new_infoset.hash).or_insert_with(|| vec![]);
                    //the reach probability will be correctly distributed across all histories in a infoset
                    //the the reach probability of the infoset is still the sum of reach probabilities of the histories
                    entry.push((subgame, prob * rp));
                }
            }
            let mut response_values = vec![];
            for (_, subgames) in subgame_map.into_iter() {
                let reward = self.search(subgames);
                response_values.push(reward);
            }
            //dbg!(&response_values);
            response_values.iter().sum()
        };
    }

    #[cfg(debug_assertions)]
    /// Verifies that all games belong to the same infoset, and we've never seen this infoset before
    fn check_infosets(&mut self, games: &Vec<(G, f32)>, player: Player) {
        let infoset = games[0].0.get_infoset(player);
        assert!(games.iter().all(|g| g.0.get_infoset(player).hash == infoset.hash));
        assert_eq!(self.seen_infosets.get(&infoset.hash), None);
        self.seen_infosets.insert(infoset.hash);
    }

    #[cfg(not(debug_assertions))]
    fn check_infosets(&mut self, games: &Vec<(G, f32)>, player: Player) {}
}