use rand::distributions::Distribution;
use rand::seq::SliceRandom;
use rand::Rng;
use std::collections::HashMap;
use std::cmp::Ordering;

use crate::game::{Game, Player};
use crate::cfr::CounterFactualRegret;

//constant in UCT
const C: f32 = 1.41;

/// TODO
/// 
/// add different averages for the two players
/// the overall exploitability will be the average of the two averages,
/// but this will let us see the advantage one player has over the other
/// against our strategy

pub struct MonteCarloTreeSearch<'a, G: Game> {
    game_constructor: Box<dyn Fn() -> G>,
    cfr: &'a CounterFactualRegret,
    on_player: Player,
    strategies: (HashMap<u64, Vec<(f32, u32)>>, HashMap<u64, Vec<(f32, u32)>>) ,
    //used to get the top level actions of the last iteration of search()
    //kind of dirty
    top_actions: Option<Vec<(f32, u32)>>,
    avg_exp_val: f32,
}

impl<'a, G: Game> MonteCarloTreeSearch<'a, G> {
    pub fn new(game_constructor: Box<dyn Fn() -> G>, cfr: &CounterFactualRegret) -> MonteCarloTreeSearch<G> {
        MonteCarloTreeSearch {
            game_constructor,
            cfr,
            on_player: Player::P1,
            strategies: (HashMap::new(), HashMap::new()),
            top_actions: None,
            avg_exp_val: -1.0,
        }
    }

    /// Runs MCTS for the given number of iterations
    ///
    /// Returns the exponential moving average of the expected value of the best inital move
    ///
    /// Repeated calls with the same instance will keep the same strategy,
    /// but the calculated exponential moving average will be reset
    pub fn run(&mut self, iterations: u32) -> f32 {
        //exponential moving average for average exploitability
        let factor = 0.99;
        for _ in 0..iterations {
            self.on_player = self.on_player.other();
            let mut game = (*self.game_constructor)();

            self.top_actions = None;
            self.search(&mut game, false);

            if let Some(top_actions) = self.top_actions.as_mut() {
                let init_value = top_actions
                    .iter()
                    .map(|(w, n)| w / (*n as f32))
                    .filter(|a| a.is_finite())
                    .max_by(|a, b| a.partial_cmp(b).unwrap_or(Ordering::Equal));

                if let Some(init_value) = init_value {
                    self.avg_exp_val = if self.avg_exp_val < 0.0 {
                        init_value
                    } else {
                        self.avg_exp_val * factor + init_value * (1.0 - factor)
                    }
                }
            }
        }
        self.avg_exp_val
    }

    fn search(&mut self, game: &mut G, rollout: bool) -> f32 {
        if let Some(reward) = game.get_reward() {
            //scale the reward space from [-1,1] to [0,1]
            return if self.on_player == Player::P1 {
                (reward + 1.0)/ 2.0
            } else {
                (-1.0 * reward + 1.0) / 2.0
            };
        }

        let (player, actions) = game.get_turn();
        let infoset = game.get_infoset(player);
        if player == self.on_player {
            //Simulation
            if rollout {
                let action = actions.choose(&mut rand::thread_rng()).unwrap();
                game.take_turn(player, &action);
                return self.search(game, rollout);
            }

            //Selection
            let strategy = player.lens(&self.strategies);
            let strat_data = strategy.get(&infoset.hash);
            let (action_index, rollout) = if let Some(strat_data) = strat_data {
                (uct(strat_data), false)
            } else {
                (rand::thread_rng().gen_range(0, actions.len()), true)
            };

            //Expansion
            game.take_turn(player, &actions[action_index]);
            let result = self.search(game, rollout);

            //Backpropagation
            let strategy = player.lens_mut(&mut self.strategies);
            let entry = strategy.entry(infoset.hash).or_insert_with(|| vec![(0.0, 0); actions.len()]);
            entry[action_index].0 += result;
            entry[action_index].1 += 1;
            if self.top_actions.is_none() {
                self.top_actions = Some(entry.clone());
            }

            result
        } else {
            //sample cfr strategy, don't record anything
            let probs = self.cfr.get_avg_strategy(player, &infoset, actions.len()).unwrap();
            let sampler = rand::distributions::WeightedIndex::new(&probs).unwrap();
            let action_index = sampler.sample(&mut rand::thread_rng());
            let action = &actions[action_index];
            game.take_turn(player, action);

            self.search(game, rollout)
        }
    }
}

/// Returns index of sampled action according to UCT
///
/// results is a vector of w, n pairs
///
/// where w is sum of the results (# of wins for binary rewards)
/// and n is the number of times the action was taken
fn uct_old(results: &Vec<(f32, u32)>) -> usize {
    let n_total: u32 = results.iter().map(|(_, n)| n).sum();
    let n_total = n_total as f32;
    //first, see if n == 0 for any action
    //which we'll take with 100% probability
    let first = results.iter()
        .enumerate()
        .find(|(_, (_, n))| *n == 0);
    if let Some((i, _)) = first {
        return i;
    }

    //pick according to UCT formula
    let (i, _) = results.iter()
        .enumerate()
        .map(|(i, (w, n))| {
            let n = *n as f32;
            (i, w / n + C * (n_total.ln() / n).sqrt())
        })
        .max_by(|(_, a) , (_, b)| a.partial_cmp(b).unwrap_or(Ordering::Equal))
        .unwrap();
    i
}
fn uct(results: &Vec<(f32, u32)>) -> usize {
    let n_total: u32 = results.iter().map(|(_, n)| n).sum();
    let n_total = n_total as f32;
    let mut best_i = 0;
    let mut best_uct = 0.0;
    for (i, (w, n)) in results.iter().enumerate() {
        if *n == 0 {
            return i;
        }
        let n = *n as f32;
        let uct_val = w / n + C * (n_total.ln() / n).sqrt();
        if uct_val > best_uct {
            best_i = i;
            best_uct = uct_val;
        }
    }
    best_i
}