use rand::distributions::Distribution;
use rand::seq::SliceRandom;
use rand::Rng;
use std::collections::HashMap;
use std::cmp::Ordering;
use std::sync::{RwLock, Mutex, Arc};

use crate::game::{Game, Player};
use crate::cfr::CounterFactualRegret;

//constant in UCT
const C: f64 = 1.41;

/// Quick and dirty way to share the MCTS strategy across threads. 
/// I'm not implementing a way to do this on disk like CFR because this shouldn't take too much memory.
///
/// Scales well if you provide enough providers to avoid contention.
pub struct StrategyProvider {
    pub strategies: (RwLock<HashMap<u64, Mutex<Vec<(f64, u64)>>>>, RwLock<HashMap<u64, Mutex<Vec<(f64, u64)>>>>),
}
impl StrategyProvider {
    pub fn new() -> StrategyProvider {
        StrategyProvider {
            strategies: (RwLock::new(HashMap::new()), RwLock::new(HashMap::new())),
        }
    }

    pub fn clear(&self) {
        self.strategies.0.write().unwrap().clear();
        self.strategies.1.write().unwrap().clear();
    }
}

pub struct MonteCarloTreeSearch<'a, G: Game> {
    game_constructor: Box<dyn Fn() -> G>,
    cfr: &'a CounterFactualRegret,
    on_player: Player,
    providers: Arc<Vec<StrategyProvider>>,
    //used to get the top level actions of the last iteration of search()
    //kind of dirty
    top_actions: Option<Vec<(f64, u64)>>,
    avg_exp_vals: (Option<f64>, Option<f64>),
    var_exp_vals: (f64, f64),
    verbose: bool,
}

impl<'a, G: Game> MonteCarloTreeSearch<'a, G> {
    pub fn new(game_constructor: Box<dyn Fn() -> G>, cfr: &CounterFactualRegret, providers: Arc<Vec<StrategyProvider>>) -> MonteCarloTreeSearch<G> {
        MonteCarloTreeSearch {
            game_constructor,
            cfr,
            providers,
            on_player: Player::P1,
            top_actions: None,
            avg_exp_vals: (None, None),
            var_exp_vals: (0.0, 0.0),
            verbose: false,
        }
    }

    pub fn set_verbose(&mut self, verbose: bool) {
        self.verbose = verbose;
    }


    /// Runs MCTS for the given number of iterations
    ///
    /// Returns the exponential moving average of the expected value of the best inital move
    ///
    /// Repeated calls with the same instance will keep the same strategy,
    /// but the calculated exponential moving average will be reset
    pub fn run(&mut self, iterations: u64) -> (f64, f64) {
        //exponential moving average for average exploitability
        let factor = 0.99;
        for i in 0..iterations {
            self.on_player = self.on_player.other();
            let mut game = (*self.game_constructor)();

            self.top_actions = None;
            self.search(&mut game, false, 0);

            if let Some(top_actions) = self.top_actions.as_mut() {
                let init_value = top_actions
                    .iter()
                    .map(|(w, n)| (w / (*n as f64)) * 2.0 - 1.0)//x*2-1 maps from [0,1] in mcts to [-1,1] in cfr
                    .filter(|a| a.is_finite())
                    .max_by(|a, b| a.partial_cmp(b).unwrap_or(Ordering::Equal));

                if let Some(init_value) = init_value {
                    let init_value = init_value as f64;
                    let exp_val = self.on_player.lens_mut(&mut self.avg_exp_vals);
                    let var = self.on_player.lens_mut(&mut self.var_exp_vals);
                    if exp_val.is_none() {
                        *exp_val = Some(init_value)
                    }

                    let x = exp_val.unwrap();
                    let delta = init_value - x;
                    *exp_val = Some(x + (1.0 - factor) * delta);
                    *var = factor * (*var + (1.0 - factor) * delta * delta);
                    if self.verbose && exp_val.is_some() {
                        println!("mcts,{},{},{},{}", self.on_player, init_value, exp_val.unwrap(), var.sqrt());
                    }
                }
            }
        }
        (self.avg_exp_vals.0.unwrap(), self.avg_exp_vals.1.unwrap())
    }

    fn search(&mut self, game: &mut G, rollout: bool, depth: u64) -> f64 {
        if let Some(reward) = game.get_reward() {
            //scale the reward space from [-1,1] to [0,1]
            return if self.on_player == Player::P1 {
                (reward + 1.0) / 2.0
            } else {
                (-1.0 * reward + 1.0) / 2.0
            } as f64;
        }

        let (player, actions) = game.get_turn();
        let infoset = game.get_infoset(player);
        if player == self.on_player {
            //Simulation
            if rollout {
                let action = actions.choose(&mut rand::thread_rng()).unwrap();
                game.take_turn(player, &action);
                return self.search(game, rollout, depth + 1);
            }

            //Selection
            let provider_index = infoset.hash as usize % self.providers.len();
            let provider = &self.providers[provider_index];
            let strategy = player.lens(&provider.strategies)
                .read().unwrap();
            let strat_data = strategy.get(&infoset.hash);
            let (action_index, rollout) = if let Some(strat_data) = strat_data {
                let strat_data = strat_data.lock().unwrap();
                (uct(&strat_data), false)
            } else {
                (rand::thread_rng().gen_range(0, actions.len()), true)
            };
            drop(strategy);

            //Expansion
            game.take_turn(player, &actions[action_index]);
            let result = self.search(game, rollout, depth + 1);

            //Backpropagation
            let provider_index = infoset.hash as usize % self.providers.len();
            let provider = &self.providers[provider_index];
            let mut strategy = player.lens(&provider.strategies)
                .write().unwrap();
            let mut entry = strategy.entry(infoset.hash).or_insert_with(|| Mutex::new(vec![(0.0, 0); actions.len()]))
                .lock().unwrap();
            entry[action_index].0 += result;
            entry[action_index].1 += 1;
            if self.top_actions.is_none() && depth == 0 {
                self.top_actions = Some(entry.clone());
            }

            result
        } else {
            //sample cfr strategy, don't record anything
            let probs = self.cfr.get_avg_strategy(player, &infoset, actions.len()).unwrap();
            let sampler = rand::distributions::WeightedIndex::new(&probs).unwrap();
            let action_index = sampler.sample(&mut rand::thread_rng());
            let action = &actions[action_index];
            game.take_turn(player, action);

            self.search(game, rollout, depth)
        }
    }
}

/// Returns index of sampled action according to UCT
///
/// results is a vector of w, n pairs
///
/// where w is sum of the results (# of wins for binary rewards)
/// and n is the number of times the action was taken
fn uct(results: &Vec<(f64, u64)>) -> usize {
    let n_total: u64 = results.iter().map(|(_, n)| n).sum();
    let n_total = n_total as f64;
    let mut best_i = 0;
    let mut best_uct = 0.0;
    for (i, (w, n)) in results.iter().enumerate() {
        if *n == 0 {
            return i;
        }
        let n = *n as f64;
        let uct_val = w / n + C * (n_total.ln() / n).sqrt();
        if uct_val > best_uct {
            best_i = i;
            best_uct = uct_val;
        }
    }
    best_i
}